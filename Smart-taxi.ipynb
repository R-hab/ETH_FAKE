{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa2PUFoxxo0+TkIMY+0Bj7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R-hab/ETH_FAKE/blob/main/Smart-taxi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "D9twIkVUOV0Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U colabgymrender\n",
        "!pip install imageio==2.4.1\n",
        "!pip install --upgrade AutoROM\n",
        "# AutoROM --accept-license\n",
        "!pip install gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9bbV5tx3QKrd",
        "outputId": "b6b4674e-f16a-43c3-fd27-88571fdb1e87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colabgymrender\n",
            "  Downloading colabgymrender-1.1.0.tar.gz (3.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.9/dist-packages (from colabgymrender) (1.0.3)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (0.4.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (1.22.4)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (4.65.0)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (2.25.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (0.1.10)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from moviepy->colabgymrender) (2.27.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.9/dist-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender) (8.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender) (2022.12.7)\n",
            "Building wheels for collected packages: colabgymrender\n",
            "  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colabgymrender: filename=colabgymrender-1.1.0-py3-none-any.whl size=3130 sha256=03391950faa528594a2cb357d59ef213d5c298562d95fbad3eb99247c7e57c1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/dc/eb/4d1d8ef70b7d696391f62606424619637bf61d6bd43f7d2298\n",
            "Successfully built colabgymrender\n",
            "Installing collected packages: colabgymrender\n",
            "Successfully installed colabgymrender-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio==2.4.1\n",
            "  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from imageio==2.4.1) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from imageio==2.4.1) (8.4.0)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303886 sha256=0f2fbb4c096133f24aaab75ffeb3d10c6e1d6cab0bb7fef737d9525f93aef5ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/44/b7/2e7cc9c5fe4a893b9cc83a010d4410557bedf6cf3b5829f497\n",
            "Successfully built imageio\n",
            "Installing collected packages: imageio\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.25.1\n",
            "    Uninstalling imageio-2.25.1:\n",
            "      Successfully uninstalled imageio-2.25.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed imageio-2.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting AutoROM\n",
            "  Downloading AutoROM-0.6.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting libtorrent\n",
            "  Downloading libtorrent-2.0.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from AutoROM) (8.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from AutoROM) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->AutoROM) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->AutoROM) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->AutoROM) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->AutoROM) (2.0.12)\n",
            "Installing collected packages: libtorrent, AutoROM\n",
            "Successfully installed AutoROM-0.6.0 libtorrent-2.0.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license,atari]) (6.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license,atari]) (1.22.4)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.9/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.27.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.3)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.6.0.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 KB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.15.0)\n",
            "Requirement already satisfied: libtorrent in /usr/local/lib/python3.9/dist-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.12)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.0-py3-none-any.whl size=446686 sha256=60bf520f34b187619d3e46c1d8de81d059be2a363d8a5964653c7ff58e192819\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/17/c9/c31922a6aaf4ec7ec90eeee5dbc40ffbaafeda64b30a208b72\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "  Attempting uninstall: autorom\n",
            "    Found existing installation: AutoROM 0.6.0\n",
            "    Uninstalling AutoROM-0.6.0:\n",
            "      Successfully uninstalled AutoROM-0.6.0\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.0 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gym[atari,accept-rom-license]==0.21.0\n"
      ],
      "metadata": {
        "id": "-wtozGRfQr1e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvirtualdisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0elUd47R8dq",
        "outputId": "59b0d3a2-0e5f-49ff-ad92-e3b5e20242cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import tensorflow as tf\n",
        "from pyvirtualdisplay import Display\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.losses import MSE\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import random\n",
        "\n",
        "Display(visible=False, size=(840, 480)).start()\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "env.reset()\n",
        "\n",
        "MEMORY_SIZE = 100_000\n",
        "GAMMA = 0.95\n",
        "ALPHA = 0.001\n",
        "NUM_STEPS_FOR_UPDATE = 4\n",
        "\n",
        "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "q_network = Sequential([\n",
        "    Input(shape=num_states),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(num_actions, activation=\"linear\")\n",
        "])\n",
        "\n",
        "target_q_network = Sequential([\n",
        "    Input(shape=num_states),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(num_actions, activation=\"linear\")\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=ALPHA)\n",
        "\n",
        "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
        "  states, actions, rewards, next_states, done_vals = experiences\n",
        "  max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
        "  y_targets = rewards + (gamma * max_qsa * (1-done_vals))\n",
        "  q_values = q_network(states)\n",
        "  q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
        "                                                tf.cast(actions, tf.int32)], axis=1))\n",
        "  loss = MSE(y_targets, q_values)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def update_target_network(q_network, target_q_network):\n",
        "  TAU=1e-3\n",
        "  for target_weights, q_network_weights in zip(target_q_network.weights, q_network.weights):\n",
        "    target_weights.assign(TAU * q_network_weights + (1.0-TAU) * target_weights)\n",
        "\n",
        "@tf.function\n",
        "def agent_learn(experiences, gamma, q_network, target_q_network, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
        "  gradients = tape.gradient(loss, q_network.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
        "  update_target_network(q_network, target_q_network)\n",
        "\n",
        "def get_action(q_values, epsilon=0):\n",
        "  if random.random() > epsilon:\n",
        "    return np.argmax(q_values.numpy()[0])\n",
        "  else:\n",
        "    return random.choice(np.arange(6))\n",
        "\n",
        "def check_update_conditions(j, NUM_STEPS_FOR_UPDATE, memory_buffer):\n",
        "  if(j+1) % NUM_STEPS_FOR_UPDATE == 0 and len(memory_buffer) > 64:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def get_experiences(memory_buffer):\n",
        "    experiences = random.sample(memory_buffer, k=64)\n",
        "    states = tf.convert_to_tensor(np.array([e.state for e in experiences if e is not None]),dtype=tf.float32)\n",
        "    actions = tf.convert_to_tensor(np.array([e.action for e in experiences if e is not None]), dtype=tf.float32)\n",
        "    rewards = tf.convert_to_tensor(np.array([e.reward for e in experiences if e is not None]), dtype=tf.float32)\n",
        "    next_states = tf.convert_to_tensor(np.array([e.next_state for e in experiences if e is not None]),dtype=tf.float32)\n",
        "    done_vals = tf.convert_to_tensor(np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
        "                                     dtype=tf.float32)\n",
        "    return (states, actions, rewards, next_states, done_vals)\n",
        "\n",
        "def get_new_epsilon(epsilon):\n",
        "  E_MIN = 0.01\n",
        "  E_DECAY = 0.05\n",
        "  return max(E_MIN, E_DECAY * epsilon)\n",
        "\n",
        "def get_one_hot_encoding(state, next_state):\n",
        "\n",
        "  state_arr = np.zeros(500)\n",
        "  next_state_arr = np.zeros(500)\n",
        "\n",
        "  state_arr[state] = 1\n",
        "  next_state_arr[next_state] = 1\n",
        "  \n",
        "  return state_arr, next_state_arr\n",
        "\n",
        "\n",
        "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
        "\n",
        "def train():\n",
        "\n",
        "  NUM_EPISODES = 50000\n",
        "  MAX_TIMESTEPS = 1000\n",
        "\n",
        "  memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
        "  target_q_network.set_weights(q_network.get_weights())\n",
        "\n",
        "  epsilon = 1.0\n",
        "\n",
        "  points_history = []\n",
        "\n",
        "  for i in range(NUM_EPISODES):\n",
        "\n",
        "    state = env.reset()\n",
        "    state, _ = get_one_hot_encoding(state, 0)\n",
        "    total_points = 0\n",
        "\n",
        "    for j in range(MAX_TIMESTEPS):\n",
        "\n",
        "      state_qn = np.expand_dims(state, axis=0)\n",
        "      q_values = q_network(state_qn)\n",
        "      action = get_action(q_values, epsilon)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      _, next_state = get_one_hot_encoding(0, next_state)\n",
        "\n",
        "      memory_buffer.append(experience(state, action, reward, next_state, done))\n",
        "\n",
        "      update = check_update_conditions(j, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
        "\n",
        "      if update:\n",
        "        experiences = get_experiences(memory_buffer)\n",
        "        agent_learn(experiences, GAMMA, q_network, target_q_network, optimizer)\n",
        "\n",
        "      state = next_state.copy()\n",
        "      total_points += reward\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    points_history.append(total_points)\n",
        "    avg_points = np.mean(points_history[-100:])\n",
        "\n",
        "    epsilon = get_new_epsilon(epsilon)\n",
        "\n",
        "    print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\", end=\"\")\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "        print(f\"\\rEpisode {i+1} | Total point average of the last {100} episodes: {avg_points:.2f}\")\n",
        "\n",
        "    if(avg_points >= 8):\n",
        "      print(f\"Environment solved in {i+1} episodes!\")\n",
        "      q_network.save('taxiye_model.h5')\n",
        "      break\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUkWO41sQt_I",
        "outputId": "1e3ce559-0199-48ba-c7ce-491e475af7ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100 | Total point average of the last 100 episodes: -211.16\n",
            "Episode 200 | Total point average of the last 100 episodes: -214.76\n",
            "Episode 300 | Total point average of the last 100 episodes: -205.31\n",
            "Episode 400 | Total point average of the last 100 episodes: -205.76\n",
            "Episode 500 | Total point average of the last 100 episodes: -205.58\n",
            "Episode 600 | Total point average of the last 100 episodes: -205.67\n",
            "Episode 700 | Total point average of the last 100 episodes: -204.75\n",
            "Episode 800 | Total point average of the last 100 episodes: -204.31\n",
            "Episode 900 | Total point average of the last 100 episodes: -206.39\n",
            "Episode 1000 | Total point average of the last 100 episodes: -205.77\n",
            "Episode 1100 | Total point average of the last 100 episodes: -203.46\n",
            "Episode 1200 | Total point average of the last 100 episodes: -204.77\n",
            "Episode 1300 | Total point average of the last 100 episodes: -187.59\n",
            "Episode 1400 | Total point average of the last 100 episodes: -184.47\n",
            "Episode 1500 | Total point average of the last 100 episodes: -171.71\n",
            "Episode 1600 | Total point average of the last 100 episodes: -171.66\n",
            "Episode 1700 | Total point average of the last 100 episodes: -182.75\n",
            "Episode 1800 | Total point average of the last 100 episodes: -182.18\n",
            "Episode 1900 | Total point average of the last 100 episodes: -146.33\n",
            "Episode 2000 | Total point average of the last 100 episodes: -169.83\n",
            "Episode 2100 | Total point average of the last 100 episodes: -157.31\n",
            "Episode 2200 | Total point average of the last 100 episodes: -122.20\n",
            "Episode 2300 | Total point average of the last 100 episodes: -111.37\n",
            "Episode 2400 | Total point average of the last 100 episodes: -96.54\n",
            "Episode 2500 | Total point average of the last 100 episodes: -89.63\n",
            "Episode 2600 | Total point average of the last 100 episodes: -83.54\n",
            "Episode 2700 | Total point average of the last 100 episodes: -70.79\n",
            "Episode 2800 | Total point average of the last 100 episodes: -64.02\n",
            "Episode 2900 | Total point average of the last 100 episodes: -62.83\n",
            "Episode 3000 | Total point average of the last 100 episodes: -64.46\n",
            "Episode 3100 | Total point average of the last 100 episodes: -39.79\n",
            "Episode 3200 | Total point average of the last 100 episodes: -82.20\n",
            "Episode 3300 | Total point average of the last 100 episodes: -63.37\n",
            "Episode 3400 | Total point average of the last 100 episodes: -67.12\n",
            "Episode 3500 | Total point average of the last 100 episodes: -61.38\n",
            "Episode 3600 | Total point average of the last 100 episodes: -50.18\n",
            "Episode 3700 | Total point average of the last 100 episodes: -62.31\n",
            "Episode 3800 | Total point average of the last 100 episodes: -69.26\n",
            "Episode 3900 | Total point average of the last 100 episodes: -67.36\n",
            "Episode 4000 | Total point average of the last 100 episodes: -56.89\n",
            "Episode 4100 | Total point average of the last 100 episodes: -69.16\n",
            "Episode 4200 | Total point average of the last 100 episodes: -47.85\n",
            "Episode 4300 | Total point average of the last 100 episodes: -57.44\n",
            "Episode 4400 | Total point average of the last 100 episodes: -44.20\n",
            "Episode 4500 | Total point average of the last 100 episodes: -45.29\n",
            "Episode 4600 | Total point average of the last 100 episodes: -62.38\n",
            "Episode 4700 | Total point average of the last 100 episodes: -47.50\n",
            "Episode 4800 | Total point average of the last 100 episodes: -44.24\n",
            "Episode 4900 | Total point average of the last 100 episodes: -50.44\n",
            "Episode 5000 | Total point average of the last 100 episodes: -52.90\n",
            "Episode 5100 | Total point average of the last 100 episodes: -43.91\n",
            "Episode 5200 | Total point average of the last 100 episodes: -36.24\n",
            "Episode 5300 | Total point average of the last 100 episodes: -45.81\n",
            "Episode 5400 | Total point average of the last 100 episodes: -46.29\n",
            "Episode 5500 | Total point average of the last 100 episodes: -38.00\n",
            "Episode 5600 | Total point average of the last 100 episodes: -42.22\n",
            "Episode 5700 | Total point average of the last 100 episodes: -37.92\n",
            "Episode 5800 | Total point average of the last 100 episodes: -45.92\n",
            "Episode 5900 | Total point average of the last 100 episodes: -39.68\n",
            "Episode 6000 | Total point average of the last 100 episodes: -40.89\n",
            "Episode 6100 | Total point average of the last 100 episodes: -54.46\n",
            "Episode 6200 | Total point average of the last 100 episodes: -53.86\n",
            "Episode 6300 | Total point average of the last 100 episodes: -61.98\n",
            "Episode 6400 | Total point average of the last 100 episodes: -58.41\n",
            "Episode 6500 | Total point average of the last 100 episodes: -32.08\n",
            "Episode 6600 | Total point average of the last 100 episodes: -49.72\n",
            "Episode 6700 | Total point average of the last 100 episodes: -62.12\n",
            "Episode 6800 | Total point average of the last 100 episodes: -46.01\n",
            "Episode 6900 | Total point average of the last 100 episodes: -41.10\n",
            "Episode 7000 | Total point average of the last 100 episodes: -52.45\n",
            "Episode 7100 | Total point average of the last 100 episodes: -55.54\n",
            "Episode 7200 | Total point average of the last 100 episodes: -41.72\n",
            "Episode 7300 | Total point average of the last 100 episodes: -54.98\n",
            "Episode 7400 | Total point average of the last 100 episodes: -53.07\n",
            "Episode 7500 | Total point average of the last 100 episodes: -30.16\n",
            "Episode 7600 | Total point average of the last 100 episodes: -37.19\n",
            "Episode 7700 | Total point average of the last 100 episodes: -52.17\n",
            "Episode 7800 | Total point average of the last 100 episodes: -18.77\n",
            "Episode 7900 | Total point average of the last 100 episodes: -30.07\n",
            "Episode 8000 | Total point average of the last 100 episodes: -33.59\n",
            "Episode 8100 | Total point average of the last 100 episodes: -41.01\n",
            "Episode 8200 | Total point average of the last 100 episodes: -38.21\n",
            "Episode 8300 | Total point average of the last 100 episodes: -46.84\n",
            "Episode 8400 | Total point average of the last 100 episodes: -41.26\n",
            "Episode 8500 | Total point average of the last 100 episodes: -30.25\n",
            "Episode 8600 | Total point average of the last 100 episodes: -34.34\n",
            "Episode 8700 | Total point average of the last 100 episodes: -36.94\n",
            "Episode 8800 | Total point average of the last 100 episodes: -17.50\n",
            "Episode 8900 | Total point average of the last 100 episodes: -43.66\n",
            "Episode 9000 | Total point average of the last 100 episodes: -27.04\n",
            "Episode 9100 | Total point average of the last 100 episodes: -35.13\n",
            "Episode 9200 | Total point average of the last 100 episodes: -17.82\n",
            "Episode 9300 | Total point average of the last 100 episodes: -20.72\n",
            "Episode 9400 | Total point average of the last 100 episodes: -11.26\n",
            "Episode 9500 | Total point average of the last 100 episodes: 1.65\n",
            "Episode 9600 | Total point average of the last 100 episodes: 4.01\n",
            "Episode 9700 | Total point average of the last 100 episodes: 3.99\n",
            "Episode 9800 | Total point average of the last 100 episodes: -1.45\n",
            "Episode 9900 | Total point average of the last 100 episodes: 3.64\n",
            "Episode 10000 | Total point average of the last 100 episodes: 5.15\n",
            "Episode 10100 | Total point average of the last 100 episodes: 3.83\n",
            "Episode 10200 | Total point average of the last 100 episodes: 7.30\n",
            "Episode 10300 | Total point average of the last 100 episodes: 7.45\n",
            "Episode 10400 | Total point average of the last 100 episodes: 6.15\n",
            "Episode 10500 | Total point average of the last 100 episodes: 7.63\n",
            "Episode 10600 | Total point average of the last 100 episodes: -2.60\n",
            "Episode 10700 | Total point average of the last 100 episodes: 7.39\n",
            "Episode 10800 | Total point average of the last 100 episodes: 7.49\n",
            "Episode 10900 | Total point average of the last 100 episodes: 5.63\n",
            "Episode 10913 | Total point average of the last 100 episodes: 8.16Environment solved in 10913 episodes!"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "import imageio"
      ],
      "metadata": {
        "id": "-xCXwz8ESHQp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "q_network = tf.keras.models.load_model('taxiye_model.h5')"
      ],
      "metadata": {
        "id": "KSmi0thxmxFj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_one_hot_encoding(state, next_state):\n",
        "\n",
        "  state_arr = np.zeros(500)\n",
        "  next_state_arr = np.zeros(500)\n",
        "\n",
        "  state_arr[state] = 1\n",
        "  next_state_arr[next_state] = 1\n",
        "  \n",
        "  return state_arr, next_state_arr\n",
        "\n",
        "def create_video(filename, env, q_network, fps=30):\n",
        "  video = imageio.get_writer(filename, fps=fps)\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  frame = env.render(mode=\"rgb_array\")\n",
        "  video.append_data(frame)\n",
        "  while not done:\n",
        "    state, _ = get_one_hot_encoding(state, 0)\n",
        "    state = np.expand_dims(state, axis=0)\n",
        "    q_values = q_network(state)\n",
        "    action = np.argmax(q_values.numpy()[0])\n",
        "    state, _, done, _ = env.step(action)\n",
        "    frame = env.render(mode=\"rgb_array\")\n",
        "    video.append_data(frame)\n",
        "    for k in range(20):\n",
        "      video.append_data(frame)\n",
        "\n",
        "filename = \"taxi.mp4\"\n",
        "\n",
        "create_video(filename, env, q_network)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqGh7FCrm5_5",
        "outputId": "13c04817-b564-454f-b8d2-f92d84c105c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/imageio/plugins/ffmpeg.py:727: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  self._proc.stdin.write(im.tostring())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFK1Cx5enHnN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}